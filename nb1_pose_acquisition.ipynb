{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bbf9f7-f8b4-4c85-8f25-579aaf0d22be",
   "metadata": {},
   "source": [
    "# Notebook 1\n",
    "# POSE Landmark acquisition and storage\n",
    "\n",
    "\n",
    "This notebook is designed to capture video and detect the pose using MediaPipe\n",
    "\n",
    "It wil capture poses of action = [TL, TR, BL, BR, CENTER]  and store it into a (directory) structure:\n",
    "\n",
    "persoon_xxxx/action/sequence_nr/frame_nr.npy\n",
    "\n",
    "where xxxx is the persons ID, sequence is a series of frames and sequence_nr and Frame_nr are the corresponing ID's of the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0c424-6f2f-44a5-b029-93c8d8ea8842",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"pose_landmarks_index.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a0bcd-5003-421a-8be5-d9ae66117564",
   "metadata": {},
   "source": [
    "The pose consists of 33 variables of (X, Y, Z, VISIBILITY).  Although all these variables are stored into the npy file, only the variables 0-12 are used in the models.\n",
    "\n",
    "pose_ml.py contains helper functions for loading, saving, displaying pose keypoints/landmarks and for handling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d449220-c055-43e5-8e18-8dd11a200c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2567510-a4d0-4451-9379-783359485019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\djase\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import winsound\n",
    "import sklearn\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    " # routines for this assignement \n",
    "import pose_ml  as pml  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a799a3-2d84-41b7-9c34-068bf18e94af",
   "metadata": {},
   "source": [
    "# Get the features and actions data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da658eda-fc59-4a21-b0e0-779e275429cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20  Features : \n",
      "(0, 2) nose, left eye\n",
      "(0, 5) nose, right eye\n",
      "(0, 7) nose, left ear\n",
      "(0, 8) nose, right ear\n",
      "(0, 9) nose, left mouth\n",
      "(0, 10) nose, right mouth\n",
      "(0, 11) nose, left shoulder\n",
      "(0, 12) nose, right shoulder\n",
      "(2, 5) left eye, right eye\n",
      "(2, 7) left eye, left ear\n",
      "(5, 8) right eye, right ear\n",
      "(7, 8) left ear, right ear\n",
      "(7, 9) left ear, left mouth\n",
      "(7, 11) Left ear, left shoulder\n",
      "(8, 10) right ear, right mouth\n",
      "(8, 12) right ear, right shoulder\n",
      "(9, 10) left mouth, right mouth\n",
      "(9, 11) left mouth, left shoulder\n",
      "(10, 12) right mouth, right shoulder\n",
      "(11, 12) left shoulder, right shoulder\n",
      "\n",
      "Actions: \n",
      "C MIDDEN\n",
      "TL LINKS BOVEN\n",
      "TR RECHTS BOVEN\n",
      "BL LINKS ONDER\n",
      "BR RECHTS ONDER\n"
     ]
    }
   ],
   "source": [
    "#get features\n",
    "features  = pml.FEATURES\n",
    "feature_descriptions = pml.FEATURE_DESCRIPTIONS\n",
    "\n",
    "# get the actions\n",
    "actions = pml.ACTIONS\n",
    "action_descriptions = pml.ACTION_DESCRIPTIONS\n",
    "\n",
    "print(len(feature_descriptions), \" Features : \")\n",
    "for key, feature in feature_descriptions.items():\n",
    "    print(key, feature)\n",
    "print(\"\\nActions: \")\n",
    "for key, action in action_descriptions.items():\n",
    "    print(key, action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dee17e-af7b-40e2-982a-e961d8bef9ce",
   "metadata": {},
   "source": [
    "set initial path's and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80a445d-c156-42c9-96e6-ca48deef9a66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m project_data \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# oritinally was ./data, was put to ./data1 to prevent poluting dataset for assingment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m graph_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "project_data = Path(r\"./data1\") # oritinally was ./data, was put to ./data1 to prevent poluting dataset for assingment\n",
    "graph_path = Path(r\"./graphs\")\n",
    "model_path = Path(r\"./models\")\n",
    "\n",
    "# 5 videos per participant\n",
    "no_sequences =5\n",
    "\n",
    "#videos have 40 frames\n",
    "sequence_length=40\n",
    "\n",
    "# change this to the ID of the webcam to be used\n",
    "web_cam = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a1198-318b-4f60-953a-343f2a963b3b",
   "metadata": {},
   "source": [
    "This routine creates a the file structure for the acquisition of data for 1 participant. It checks for existing data and appends it to a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b835f2-d51f-4510-b5e3-0add7b2852bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# object creation for MediaPipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586c8d01-ec36-43ea-a04f-9545537b8e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# detection of the landmarks from the MediaPipe model\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable =False\n",
    "    results =  model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d41e4-c109-4d01-ba75-b2f9ea82a84e",
   "metadata": {},
   "source": [
    "Create a folder structure to store the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ab46a5-d51b-4589-b8d8-82364662883d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_person_structure(actions):\n",
    "\n",
    "    folder_list = {}\n",
    "    folders = [x for x in project_data.iterdir() if x.is_dir()]\n",
    "    if len(folders)>1:\n",
    "        seq_nr= int(str(folders[-1]).split(\"_\")[-1])+1\n",
    "    else:\n",
    "        seq_nr=0\n",
    "   \n",
    "    person_folder= project_data / f\"person_{seq_nr:04d}\"\n",
    "    folder_list['PERSON']= person_folder\n",
    "    for action in actions:\n",
    "        action_folder = person_folder / action\n",
    "        #action_folder.mkdir(parents=True, exist_ok=False)\n",
    "        folder_list[action]=action_folder\n",
    "    return folder_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3088aa-338a-45e8-993e-4dbf895d65f2",
   "metadata": {},
   "source": [
    "This routine checks the webcam and determines the frame rate\n",
    "The frame rate should not be less than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b063de-71a2-4174-9518-9f3a91f18bca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good frame rate: 26.8 fps!\n"
     ]
    }
   ],
   "source": [
    "def test_webcam():\n",
    "    # determine frame rate of camera\n",
    "    cap = cv2.VideoCapture(web_cam, cv2.CAP_DSHOW)\n",
    "\n",
    "    if cap.isOpened():\n",
    "        num_frames = 60\n",
    "        start = time.time()\n",
    "        for i in range(0, num_frames):\n",
    "            ret, frame = cap.read()\n",
    "        end = time.time()\n",
    "        fps = num_frames / (end - start)\n",
    "        if fps >20:\n",
    "            print(f\"Good frame rate: {fps:.1f} fps!\")\n",
    "        else:\n",
    "            print(f\"This webcam has a low frame rate: {fps:.1f} fps! Please consider another webcam!\")\n",
    "    else:\n",
    "        print(\"Cannot open webcam\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "test_webcam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad53f92-0417-48c5-bf6e-d407b9d86998",
   "metadata": {},
   "source": [
    "This routine is testing the acquisition and landmark detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e01a3c-6867-4b17-875a-d25c7df0951b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# webcam used for the assignemet is max 1920 x 1080\n",
    "# this is quite large so reducing to halve the size. \n",
    "# change this for other formats.\n",
    "cap = cv2.VideoCapture(web_cam, cv2.CAP_DSHOW) \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920/2)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080/2)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        pml.draw_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow(\"Camera feed:\", image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335eaff2-41dc-4fe9-98dc-748186761f2d",
   "metadata": {},
   "source": [
    "Sounds helping participants indicating start - stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d352a3-0e04-4840-921a-fbb3024ff717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beep_start():\n",
    "    frequency = 2500  # 2500 Hz for start\n",
    "    duration = 500   # 1000 ms = 1 second\n",
    "    winsound.Beep(frequency, duration)\n",
    "\n",
    "def beep_stop():\n",
    "    frequency = 1200  # 1200 Hz for stop\n",
    "    duration = 500   # 1000 ms = 1 second\n",
    "    winsound.Beep(frequency, duration)\n",
    "# beep_start()\n",
    "# beep_stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e65cd7-f487-4987-86a4-8689855a34dd",
   "metadata": {},
   "source": [
    "Selects random actions such that all classes are sampled 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b7cf209-e0b1-4164-af81-25193220aec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BR', 'TL', 'C', 'C', 'TL', 'BR', 'BR', 'BL', 'C', 'BL', 'TR', 'BL', 'TL', 'C', 'BL', 'TR', 'BL', 'TL', 'TL', 'TR', 'BR', 'BR', 'C', 'TR', 'TR']\n"
     ]
    }
   ],
   "source": [
    "def create_random_action(actions, repeats):\n",
    "    random_list = actions.copy()*repeats\n",
    "    random.shuffle(random_list)\n",
    "    return random_list\n",
    "  \n",
    "print(create_random_action(['C','TL','TR','BL','BR'],5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee969dc1-06e6-4e20-85b2-ae39c18f3d82",
   "metadata": {},
   "source": [
    "# Main sample acquisition routine\n",
    "# Run this routine for every participant \n",
    "\n",
    "1. creates a folder structur for the new participant\n",
    "2. selects randomly actions to sample each class 5x \n",
    "3. presents the action in the screen.\n",
    "4. Starts with a high frequency beep to indicate start of an action\n",
    "5. Beeps at a low frequencey to indicate stop of an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107ad1a2-9c9a-4259-910b-3710d497061f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TR', 'TR', 'BR', 'C', 'TL', 'BR', 'BL', 'TL', 'BR', 'C', 'C', 'TL', 'C', 'BR', 'TL', 'TL', 'BR', 'BL', 'BL', 'TR', 'BL', 'TR', 'C', 'BL', 'TR']\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(web_cam, cv2.CAP_DSHOW) \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920/2)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080/2)\n",
    "\n",
    "folder_structure = create_person_structure(actions)\n",
    "random_actions = create_random_action(actions, no_sequences)\n",
    "print(random_actions)\n",
    "break_flag=False\n",
    "sequences = {key: 0 for key in actions} # containing the amount of sequence per action \n",
    "counter = 0;\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        # loop trhough actions\n",
    "    for action in random_actions:\n",
    "        # Loop through sequence aka videos\n",
    "        sequence=  sequences[action] \n",
    "       \n",
    "\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            pml.draw_landmarks(image, results)\n",
    "            \n",
    "            # write text for participant and beep!\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, f'KIJK NAAR {action_descriptions[action]}', (120, 200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}/{frame_num}/   {counter}', (20, 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                cv2.imshow(\"Camera feed:\", image)\n",
    "\n",
    "                cv2.waitKey(150)\n",
    "                beep_start()\n",
    "            elif frame_num == sequence_length -1:\n",
    "                beep_stop()\n",
    "                #pass\n",
    "            else:\n",
    "                cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}/{frame_num}/   {counter}', (20, 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.imshow(\"Camera feed:\", image)\n",
    "\n",
    "            key_points = pml.extract_keypoints(results)\n",
    "            \n",
    "            # save teh file\n",
    "            npy_path = folder_structure[action] / str(sequence) \n",
    "            if not npy_path.exists():\n",
    "                npy_path.mkdir(parents=True, exist_ok=False)            \n",
    "            npy_file = '.\\\\'+ str(npy_path / str(frame_num))\n",
    "            np.save(npy_file, key_points)\n",
    "           \n",
    "        sequences[action] = sequence + 1\n",
    "            counter += 1\n",
    "\n",
    "            if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "                break_flag=True\n",
    "                break\n",
    "        if break_flag:\n",
    "            break\n",
    "    \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
